{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3024395e",
   "metadata": {},
   "source": [
    "# xSiGra model explanations detailed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72858c3",
   "metadata": {},
   "source": [
    "## Here we show how to compute explanation for 1 fov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63235f0",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ed4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../xSiGra_model')\n",
    "from utils import Cal_Spatial_Net\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33151d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from train_transformer import get_explanations, train_nano_fov, test_nano_fov\n",
    "from transModel import ClusteringLayer\n",
    "from utils import Cal_Spatial_Net, Stats_Spatial_Net, _hungarian_match\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"1234\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a808d9b",
   "metadata": {},
   "source": [
    "## Set hyperparamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729fb46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--root', type=str, default='../dataset/nanostring/lung13')\n",
    "parser.add_argument('--epochs', type=int, default=200)\n",
    "parser.add_argument('--id', type=str, default='fov1')\n",
    "parser.add_argument('--img_name', type=str, default='F001')\n",
    "parser.add_argument('--neurons', type=str, default='512,30')\n",
    "parser.add_argument('--num_layers', type=str, default=\"2\")\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--save_path', type=str, default='../checkpoint_fov1/nanostring_train_lung13_tutorial')\n",
    "parser.add_argument('--ncluster', type=int, default=8)\n",
    "parser.add_argument('--repeat', type=int, default=1)\n",
    "parser.add_argument('--use_gray', type=float, default=0)\n",
    "parser.add_argument('--test_only', type=int, default=1)\n",
    "parser.add_argument('--pretrain', type=str, default='best.pth')\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "parser.add_argument('--img_size', type=str, default='60,60')\n",
    "parser.add_argument('--cluster_method', type=str, default='leiden')\n",
    "parser.add_argument('--num_fov', type=int, default=1)\n",
    "parser.add_argument('--dataset', type=str, default='lung13')\n",
    "parser.add_argument('--benchmark', type=str, default='gradcam')\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "\n",
    "root = opt.root\n",
    "seed = opt.seed\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5666de",
   "metadata": {},
   "source": [
    "## Generate adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705570bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dataset/nanostring/lung13/fov1/CellComposite_F001.jpg\n",
      "------Calculating spatial graph...\n",
      "The graph contains 12930 edges, 3332 cells.\n",
      "3.8806 neighbors per cell on average.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gen_adatas(root, id, img_name):\n",
    "\n",
    "    adata = sc.read(os.path.join(root, id, 'sampledata.h5ad'))\n",
    "    adata.var_names_make_unique()\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "    print(os.path.join(root, id, 'CellComposite_%s.jpg'%(img_name)))\n",
    "    img = cv2.imread(os.path.join(root, id, 'CellComposite_%s.jpg'%(img_name)))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if opt.use_gray:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    transform = transforms.ToTensor()\n",
    "    img = transform(img)\n",
    "\n",
    "    patchs = []\n",
    "\n",
    "    w, h = opt.img_size.split(\",\")\n",
    "    w = int(w)\n",
    "    h = int(h)\n",
    "\n",
    "    for coor in adata.obsm['spatial']:\n",
    "        x, y = coor\n",
    "        img_p = img[:, int(y-h):int(y+h), int(x-w): int(x+w)]\n",
    "        patchs.append(img_p.flatten()) # 4 * h * w\n",
    "    patchs = np.stack(patchs)\n",
    "\n",
    "    df = pd.DataFrame(patchs, index=adata.obs.index)\n",
    "    adata.obsm['imgs'] = df\n",
    "\n",
    "    Cal_Spatial_Net(adata, rad_cutoff=80)\n",
    "    Stats_Spatial_Net(adata)\n",
    "    return adata\n",
    "\n",
    "ids = ['fov'+str(i) for i in range(1,int(opt.num_fov)+1)]\n",
    "img_names = ['F00'+str(i) for i in range(1,10)]\n",
    "img_names = img_names + ['F0'+str(i) for i in range(10,int(opt.num_fov)+1)]\n",
    "\n",
    "adatas = list()\n",
    "for id, name in zip(ids, img_names):\n",
    "    adata = gen_adatas(opt.root, id, name)\n",
    "    adatas.append(adata)\n",
    "\n",
    "sp = os.path.join(opt.save_path, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c3c29",
   "metadata": {},
   "source": [
    "## Train model for fov1 or use your saved checkpoints for fov1 and directly test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bbb9612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██| 200/200 [00:43<00:00,  4.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3332 × 980\n",
       "    obs: 'cell_type', 'cx', 'cy', 'cx_g', 'cy_g', 'merge_cell_type'\n",
       "    uns: 'log1p', 'Spatial_Net'\n",
       "    obsm: 'spatial', 'spatial_global', 'imgs'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nano_fov(\n",
    "    opt,\n",
    "    adatas,\n",
    "    hidden_dims=opt.neurons,\n",
    "    n_epochs=opt.epochs,\n",
    "    lr=opt.lr,\n",
    "    random_seed=opt.seed,\n",
    "    save_path=sp,\n",
    "    repeat=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0da934",
   "metadata": {},
   "source": [
    "## Choose best model for fov1 or use your saved anndata for fov1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893fd8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_191_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 21:21:31.648386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-14 21:21:34.990836: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "0.5625\n",
      "0.46875\n",
      "0.421875\n",
      "0.3984375\n",
      "0.38671875\n",
      "0.392578125\n",
      "find 0.392578125\n",
      "Davies_bouldin: 1.78\n",
      "ARI: 0.45\n",
      "ARI: 0.45\n",
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_192_0.pth\n",
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "find 0.375\n",
      "Davies_bouldin: 1.79\n",
      "ARI: 0.53\n",
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_193_0.pth\n",
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "0.5625\n",
      "0.46875\n",
      "0.421875\n",
      "0.3984375\n",
      "0.41015625\n",
      "find 0.41015625\n",
      "Davies_bouldin: 1.93\n",
      "ARI: 0.41\n",
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_194_0.pth\n",
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "0.5625\n",
      "0.46875\n",
      "0.421875\n",
      "find 0.421875\n",
      "Davies_bouldin: 1.75\n",
      "ARI: 0.46\n",
      "ARI: 0.46\n",
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_195_0.pth\n",
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "0.5625\n",
      "0.46875\n",
      "0.421875\n",
      "0.3984375\n",
      "0.38671875\n",
      "find 0.38671875\n",
      "Davies_bouldin: 1.86\n",
      "ARI: 0.53\n",
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_196_0.pth\n",
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "0.5625\n",
      "0.46875\n",
      "0.421875\n",
      "0.3984375\n",
      "find 0.3984375\n",
      "Davies_bouldin: 1.79\n",
      "ARI: 0.52\n",
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_197_0.pth\n",
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "0.5625\n",
      "0.46875\n",
      "0.421875\n",
      "0.3984375\n",
      "0.38671875\n",
      "0.380859375\n",
      "0.3837890625\n",
      "0.38232421875\n",
      "find 0.38232421875\n",
      "Davies_bouldin: 1.83\n",
      "ARI: 0.52\n",
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_198_0.pth\n",
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "find 0.375\n",
      "Davies_bouldin: 1.68\n",
      "ARI: 0.57\n",
      "ARI: 0.57\n",
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_199_0.pth\n",
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "find 0.375\n",
      "Davies_bouldin: 1.84\n",
      "ARI: 0.41\n",
      "../checkpoint_fov1/nanostring_train_lung13_tutorial/all/final_200_0.pth\n",
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "0.1875\n",
      "0.28125\n",
      "0.328125\n",
      "0.3515625\n",
      "0.36328125\n",
      "find 0.36328125\n",
      "Davies_bouldin: 1.75\n",
      "ARI: 0.55\n",
      "Select model time required in hours: 0.07481494261158837\n"
     ]
    }
   ],
   "source": [
    "adata_pred = test_nano_fov(\n",
    "    opt,\n",
    "    adatas,\n",
    "    hidden_dims=opt.neurons,\n",
    "    random_seed=opt.seed,\n",
    "    save_path=sp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb5732",
   "metadata": {},
   "source": [
    "## Compute ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8916435",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata_pred, opt.ncluster, use_rep=\"pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b26d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def res_search(adata_pred, ncluster, seed, iter=200):\n",
    "    start = 0\n",
    "    end = 3\n",
    "    i = 0\n",
    "    while start < end:\n",
    "        if i >= iter:\n",
    "            return res\n",
    "        i += 1\n",
    "        res = (start + end) / 2\n",
    "\n",
    "        random.seed(seed)\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        sc.tl.leiden(adata_pred, random_state=seed, resolution=res)\n",
    "        count = len(set(adata_pred.obs[\"leiden\"]))\n",
    "        if count == ncluster:\n",
    "            return res\n",
    "        if count > ncluster:\n",
    "            end = res\n",
    "        else:\n",
    "            start = res\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Perform leiden clustering\n",
    "res = res_search(adata_pred, opt.ncluster, opt.seed)\n",
    "sc.tl.leiden(adata_pred, resolution=res, key_added=\"leiden\", random_state=opt.seed)\n",
    "obs_df = adata_pred.obs.dropna()\n",
    "\n",
    "ARI = adjusted_rand_score(obs_df['leiden'], obs_df['merge_cell_type'])\n",
    "print('ARI: %.2f'%ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c8257",
   "metadata": {},
   "source": [
    "## Train linear layer to convert leiden clusters to probability using celltype as ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7608d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['endothelial' 'epithelial' 'fibroblast' 'lymphocyte' 'mast' 'myeloid'\n",
      " 'neutrophil' 'tumors']\n",
      "2.062751054763794\n",
      "0.6849991083145142\n",
      "\n",
      "train loss: 0.9720, train acc: 73.7711\n",
      "0.5046316981315613\n",
      "0.44000017642974854\n",
      "\n",
      "train loss: 0.7414, train acc: 82.9644\n",
      "0.4778059720993042\n",
      "0.26733288168907166\n",
      "\n",
      "train loss: 0.6270, train acc: 86.0413\n",
      "0.36947762966156006\n",
      "0.26081836223602295\n",
      "\n",
      "train loss: 0.5588, train acc: 86.8293\n",
      "0.2874548137187958\n",
      "0.3590066432952881\n",
      "\n",
      "train loss: 0.5109, train acc: 87.6548\n",
      "0.2807098925113678\n",
      "0.37260568141937256\n",
      "\n",
      "train loss: 0.4756, train acc: 88.6679\n",
      "0.28875651955604553\n",
      "0.23319965600967407\n",
      "\n",
      "train loss: 0.4474, train acc: 89.9812\n",
      "0.36375588178634644\n",
      "0.20894473791122437\n",
      "\n",
      "train loss: 0.4256, train acc: 89.9062\n",
      "0.27625173330307007\n",
      "0.2906970977783203\n",
      "\n",
      "train loss: 0.4073, train acc: 90.0188\n",
      "0.1800493746995926\n",
      "0.28835874795913696\n",
      "\n",
      "train loss: 0.3916, train acc: 90.7317\n",
      "0.21729464828968048\n",
      "0.2582908868789673\n",
      "\n",
      "train loss: 0.3782, train acc: 90.9944\n",
      "0.18634077906608582\n",
      "0.2653675377368927\n",
      "\n",
      "train loss: 0.3666, train acc: 90.8443\n",
      "0.20412082970142365\n",
      "0.1644822508096695\n",
      "\n",
      "train loss: 0.3562, train acc: 91.2570\n",
      "0.14332571625709534\n",
      "0.2577141225337982\n",
      "\n",
      "train loss: 0.3466, train acc: 91.6323\n",
      "0.18827585875988007\n",
      "0.2433646321296692\n",
      "\n",
      "train loss: 0.3382, train acc: 91.6323\n",
      "0.19057883322238922\n",
      "0.21416059136390686\n",
      "\n",
      "train loss: 0.3305, train acc: 92.2326\n",
      "0.1752551645040512\n",
      "0.15739192068576813\n",
      "\n",
      "train loss: 0.3231, train acc: 92.2326\n",
      "0.19357331097126007\n",
      "0.2170362025499344\n",
      "\n",
      "train loss: 0.3164, train acc: 92.6079\n",
      "0.17652758955955505\n",
      "0.1618821620941162\n",
      "\n",
      "train loss: 0.3103, train acc: 92.2702\n",
      "0.22572584450244904\n",
      "0.1761486828327179\n",
      "\n",
      "train loss: 0.3046, train acc: 92.9081\n",
      "0.2347506433725357\n",
      "0.1317168027162552\n",
      "\n",
      "train loss: 0.2994, train acc: 92.9831\n",
      "0.2126762866973877\n",
      "0.24187391996383667\n",
      "\n",
      "train loss: 0.2944, train acc: 92.6829\n",
      "0.15472465753555298\n",
      "0.15939505398273468\n",
      "\n",
      "train loss: 0.2895, train acc: 93.1707\n",
      "0.15305234491825104\n",
      "0.14091256260871887\n",
      "\n",
      "train loss: 0.2849, train acc: 93.4334\n",
      "0.16214986145496368\n",
      "0.2065245807170868\n",
      "\n",
      "train loss: 0.2807, train acc: 93.3583\n",
      "0.17861878871917725\n",
      "0.20222246646881104\n",
      "\n",
      "train loss: 0.2765, train acc: 93.7711\n",
      "0.12592986226081848\n",
      "0.2235907018184662\n",
      "\n",
      "train loss: 0.2726, train acc: 93.8837\n",
      "0.24580302834510803\n",
      "0.1740923970937729\n",
      "\n",
      "train loss: 0.2688, train acc: 94.1839\n",
      "0.141043022274971\n",
      "0.13592950999736786\n",
      "\n",
      "train loss: 0.2652, train acc: 93.9587\n",
      "0.18162082135677338\n",
      "0.20502449572086334\n",
      "\n",
      "train loss: 0.2618, train acc: 94.2964\n"
     ]
    }
   ],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, indata, label):\n",
    "        self.indata = indata\n",
    "        self.label = [int(item) for item in label.values.tolist()]\n",
    "        self.label = np.array(self.label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indata)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indata = torch.tensor(self.indata[index])\n",
    "        label = torch.tensor(self.label[index])\n",
    "        return indata, label\n",
    "\n",
    "cross_el = torch.nn.CrossEntropyLoss()\n",
    "obs_df = adata_pred.obs.dropna()\n",
    "\n",
    "device = torch.device(opt.device if torch.cuda.is_available() else \"cpu\")\n",
    "linmodel = ClusteringLayer(opt.ncluster).to(device)\n",
    "\n",
    "labels = obs_df[\"merge_cell_type\"].tolist()\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "gt = le.fit_transform(labels)\n",
    "print(le.classes_)\n",
    "y = pd.DataFrame(gt, columns =['merge_cell_type'])\n",
    "\n",
    "dataset = Custom_Dataset(adata_pred.obsm['pred'], y['merge_cell_type'])\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(linmodel.parameters(), lr=0.001)\n",
    "epochs = 30\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    linmodel.train()\n",
    "    correct = 0\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        data, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        out = linmodel(data.float().to(device))\n",
    "        loss = cross_el(out, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        x = torch.nn.functional.log_softmax(out,dim = 1)\n",
    "        _,pred = torch.max(x, dim=1)\n",
    "        correct += torch.sum(pred==labels.to(device)).item()\n",
    "        total += labels.size(0)\n",
    "        if (idx) % 20 == 0:\n",
    "            print(loss.item()) \n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n",
    "\n",
    "if not os.path.exists(os.path.join(\"../saved_model_fov1/\" + opt.dataset)):\n",
    "    os.makedirs(os.path.join(\"../saved_model_fov1/\" + opt.dataset))    \n",
    "torch.save(linmodel.state_dict(), '../saved_model_fov1/'+opt.dataset+'/linmodel_gt.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b068c",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678cad3",
   "metadata": {},
   "source": [
    "## Match clusters to cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5397f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test acc: 87.7061\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "running_loss = 0.0\n",
    "total = 0\n",
    "total_step = len(test_loader)\n",
    "\n",
    "linmodel = ClusteringLayer(opt.ncluster).to(device)\n",
    "linmodel.load_state_dict(torch.load('../saved_model_fov1/'+opt.dataset+'/linmodel_gt.pth'))\n",
    "linmodel = linmodel.to(device)\n",
    "linmodel.eval()\n",
    "\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    data, labels = batch\n",
    "    out = linmodel(data.float().to(device))\n",
    "    x = torch.nn.functional.log_softmax(out,dim = 1)\n",
    "    _,pred = torch.max(x, dim=1)\n",
    "    correct += torch.sum(pred==labels.to(device)).item()\n",
    "    total += labels.size(0)\n",
    "    \n",
    "test_acc = 100 * correct / total\n",
    "print(f'\\ntest acc: {(100 * correct / total):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4046e4",
   "metadata": {},
   "source": [
    "## Load VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e7804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "\n",
    "# Pretrained VGG-16\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.features = list(model.features)\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "        self.pooling = model.avgpool\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = model.classifier[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.pooling(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "vgg_model = FeatureExtractor(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14919150",
   "metadata": {},
   "source": [
    "## Compute explanations\n",
    "\n",
    "### Cluster explanations with cell and gene importance scores to identify the particular cluster are stored in ../cluster_results_gradcam_fov1_gt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "156102a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5d1cc",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a01af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "start_time = time.time()\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "355917f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import anndata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy.sparse as sp\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from torch import nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from transModel import xSiGraModel, TransImg\n",
    "from utils import Transfer_img_Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68769946",
   "metadata": {},
   "source": [
    "## Process adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48398c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.pretrain = \"best.pth\"\n",
    "save_path=opt.save_path + \"/all/\"\n",
    "datas = []\n",
    "gene_dim = 0\n",
    "img_dim = 0\n",
    "\n",
    "w, h = opt.img_size.split(\",\")\n",
    "w = int(w)\n",
    "h = int(h)\n",
    "\n",
    "device = torch.device(opt.device if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.vgg16(pretrained=True)\n",
    "vgg_model = FeatureExtractor(model)\n",
    "vgg_model = vgg_model.to(device)\n",
    "\n",
    "for adata in adatas:\n",
    "    import scipy.sparse as sp\n",
    "    adata.X = sp.csr_matrix(adata.X)\n",
    "    data, img = Transfer_img_Data(adata)\n",
    "    gene_dim = data.x.shape[1]\n",
    "    img_dim = img.x.shape[1]\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    img_transformed = img.x.numpy()\n",
    "\n",
    "    for i in range(img_transformed.shape[0]):\n",
    "        img = img_transformed[i].reshape(w * 2, h * 2, 3)\n",
    "        img = transform(np.uint8(img))\n",
    "        img = img.reshape(1, 3, 224, 224)\n",
    "        img = img.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature = vgg_model(img)\n",
    "\n",
    "        img = feature\n",
    "        features.append(img.cpu().detach().numpy().reshape(-1))\n",
    "\n",
    "    features = np.array(features)\n",
    "    features = np.vstack(features).astype(np.float)\n",
    "    img = torch.from_numpy(features)\n",
    "\n",
    "    img_dim = features.shape[1]\n",
    "\n",
    "    hidden_dims = opt.neurons\n",
    "    hidden_dims_arr = hidden_dims.split(\",\")\n",
    "    arr = []\n",
    "    for i in hidden_dims_arr:\n",
    "        arr.append(int(i))\n",
    "    hidden_dims = arr\n",
    "\n",
    "    data.x = torch.cat([data.x, img], dim=1)\n",
    "    datas.append(data)\n",
    "\n",
    "adata = anndata.concat(adatas)\n",
    "loader = DataLoader(datas, batch_size=1, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7aaac5",
   "metadata": {},
   "source": [
    "## Create model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07523d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransImg(hidden_dims=[gene_dim, img_dim] + hidden_dims).to(device)\n",
    "\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(save_path, opt.pretrain), map_location=device)\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "clustermodel = xSiGraModel(\n",
    "    hidden_dims=[gene_dim, img_dim] + hidden_dims,\n",
    "    pretrained_model=model,\n",
    "    num_clusters=opt.ncluster,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da228ae",
   "metadata": {},
   "source": [
    "## Load classification layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0743dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustermodel.out1.weight.data = linmodel.out1.weight.data.to(device)\n",
    "clustermodel.out2.weight.data = linmodel.out2.weight.data.to(device)\n",
    "clustermodel.out1.bias.data = linmodel.out1.bias.data.to(device)\n",
    "clustermodel.out2.bias.data = linmodel.out2.bias.data.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd46220b",
   "metadata": {},
   "source": [
    "## Freeze model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a61acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in clustermodel.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "clustermodel.out1.weight.requires_grad = False\n",
    "clustermodel.out1.bias.requires_grad = False\n",
    "clustermodel.out2.weight.requires_grad = False\n",
    "clustermodel.out2.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f8343",
   "metadata": {},
   "source": [
    "## Functions to compute cell-level and gene-level explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "625e9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_explanations(device, model, cluster, out, bgene, edge_index):\n",
    "    model.eval()\n",
    "    vsize = out.size()\n",
    "    device = device\n",
    "    v = torch.rand(vsize).to(device)\n",
    "    grd = torch.zeros(vsize).to(device)\n",
    "    out.to(device)\n",
    "\n",
    "    # For each cluster backtrack to get gradients wrt gene expression input\n",
    "    grd[:, cluster] = 1\n",
    "    out.backward(gradient=grd, retain_graph=True)\n",
    "\n",
    "    final_conv_grads = model.pretrained_model.get_activations_gradient()\n",
    "    final_conv_acts = model.pretrained_model.get_activations(bgene, edge_index).detach()\n",
    "    bgene.grad.zero_()\n",
    "    grad_cam_weights, gene_cam_weights = grad_cam(final_conv_acts, final_conv_grads)\n",
    "    return grad_cam_weights, gene_cam_weights\n",
    "\n",
    "\n",
    "def grad_cam(final_conv_acts, final_conv_grads):\n",
    "    node_heat_map = []\n",
    "    gene_heat_map = []\n",
    "    alphas = torch.mean(final_conv_grads, axis=0)\n",
    "    gene_heat_map = F.relu(final_conv_grads * final_conv_acts)\n",
    "\n",
    "    node_heat_map = F.relu(\n",
    "        (final_conv_grads * final_conv_acts).sum(dim=1, keepdims=True)\n",
    "    )\n",
    "    return node_heat_map, gene_heat_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d027150",
   "metadata": {},
   "source": [
    "## Get cluster predictions and Compute explanations for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26bf7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_weights = {}\n",
    "cluster_weights_gene = {}\n",
    "\n",
    "for i in range(0, opt.ncluster):\n",
    "    cluster_weights[i] = None\n",
    "\n",
    "for i in range(0, opt.ncluster):\n",
    "    cluster_weights_gene[i] = None\n",
    "\n",
    "# Stores the cluster label for each cell\n",
    "outlabel = []\n",
    "start_time = time.time()\n",
    "for i, batch in enumerate(loader):\n",
    "    batch = batch.to(device)\n",
    "    bgene = batch.x[:, :gene_dim]\n",
    "    bimg = batch.x[:, gene_dim:]\n",
    "    edge_index = batch.edge_index\n",
    "\n",
    "    bgene = bgene.float()\n",
    "    bgene.requires_grad = True\n",
    "    bimg = bimg.float()\n",
    "    clusterlabel = clustermodel(\n",
    "        bgene.float(), bimg.float(), edge_index, batch.batch\n",
    "    )\n",
    "\n",
    "    for label in clusterlabel:\n",
    "        _, pred = torch.max(label, dim=0)\n",
    "        outlabel.append(pred.item())\n",
    "\n",
    "    # For each cluster get node and gene importance scores\n",
    "    for k in range(0, opt.ncluster):\n",
    "        node_explain, gene_explain = compute_explanations(\n",
    "            device, clustermodel, k, clusterlabel, bgene.float(), edge_index\n",
    "        )\n",
    "        if cluster_weights[k] is None:\n",
    "            cluster_weights[k] = node_explain\n",
    "        else:\n",
    "            cluster_weights[k] = torch.cat((cluster_weights[k], node_explain))\n",
    "\n",
    "        if cluster_weights_gene[k] is None:\n",
    "            cluster_weights_gene[k] = gene_explain\n",
    "        else:\n",
    "            cluster_weights_gene[k] = torch.cat(\n",
    "                (cluster_weights_gene[k], gene_explain)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a01f0d",
   "metadata": {},
   "source": [
    "## Store explanations in csv files for each cluster\n",
    "\n",
    "### To use for other  specific fovs say 14 or group of fovs like 5-10 you need to modify the indices start and end to pick the correct metadata from the anndata file and fov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10815769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream task time in hours: 0.02216945250829061\n"
     ]
    }
   ],
   "source": [
    "labels = adata.obs[\"merge_cell_type\"]\n",
    "\n",
    "def _hungarian_match(flat_preds, flat_target, preds_k, target_k):\n",
    "    num_samples = flat_target.shape[0]\n",
    "    num_k = preds_k\n",
    "    num_correct = np.zeros((num_k, num_k))\n",
    "    for c1 in range(num_k):\n",
    "        for c2 in range(num_k):\n",
    "            votes = int(((flat_preds == c1) * (flat_target == c2)).sum())\n",
    "            num_correct[c1, c2] = votes\n",
    "    match = linear_sum_assignment(num_samples - num_correct)\n",
    "    match = np.array(list(zip(*match)))\n",
    "    res = []\n",
    "    for out_c, gt_c in match:\n",
    "        res.append((out_c, gt_c))\n",
    "    return res\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "gt = le.fit_transform(labels)\n",
    "\n",
    "leiden = np.array(outlabel)\n",
    "match = _hungarian_match(\n",
    "    leiden.astype(np.int8), gt.astype(np.int8), opt.ncluster, 8\n",
    ")\n",
    "\n",
    "# Maps cluster number to cell type using humgarian matching\n",
    "dict_cluster_map_cell = {}\n",
    "for mapping in match:\n",
    "    dict_cluster_map_cell[mapping[0]] = le.classes_[mapping[1]]\n",
    "\n",
    "for i in range(0, opt.ncluster):\n",
    "    adata.obs[\"cmap\" + str(i)] = np.array(cluster_weights[i].cpu())\n",
    "for i in range(0, opt.ncluster):\n",
    "    adata.uns[\"gmap\" + str(i)] = np.array(cluster_weights_gene[i].cpu())\n",
    "keys_list = [\"cmap\" + str(i) for i in range(opt.ncluster)]\n",
    "genedf = sc.get.obs_df(adata, keys=keys_list)\n",
    "\n",
    "# Used to fetch proper metadata from anndata file\n",
    "count = 0\n",
    "start = 0\n",
    "\n",
    "ids = [\"fov\" + str(i) for i in range(1, int(opt.num_fov) + 1)]\n",
    "img_names = [\"F00\" + str(i) for i in range(1, 10)]\n",
    "img_names = img_names + [\"F0\" + str(i) for i in range(10, int(opt.num_fov) + 1)]\n",
    "\n",
    "datas = []\n",
    "adata_pred = adata\n",
    "\n",
    "all_cells_df = pd.DataFrame()\n",
    "fov = 0\n",
    "fovs = []\n",
    "dict_cmap = {}\n",
    "gt = []\n",
    "for i in range(0, opt.ncluster):\n",
    "    dict_cmap[\"cmap\" + str(i)] = []\n",
    "\n",
    "# For each FOV\n",
    "for id in ids:\n",
    "    fov += 1\n",
    "\n",
    "    adata = sc.read(\n",
    "        os.path.join(\"../dataset/nanostring/\" + opt.dataset + \"/\", id, \"sampledata.h5ad\")\n",
    "    )\n",
    "    \n",
    "    # Update to select metadata from next fov\n",
    "    start = count\n",
    "    end = start + adata.shape[0]\n",
    "    count = count + adata.shape[0]\n",
    "\n",
    "    adata.var_names_make_unique()\n",
    "    \n",
    "    # Store gt for all fovs\n",
    "    gt = gt + adata.obs[\"merge_cell_type\"].tolist()\n",
    "    for i in range(opt.ncluster):\n",
    "        adata.uns[\"gmap\" + str(i)] = adata_pred.uns[\"gmap\" + str(i)]\n",
    "        \n",
    "        # Select for particular fov\n",
    "        adata.obs[\"cmap\" + str(i)] = genedf.iloc[start:end][\"cmap\" + str(i)]\n",
    "        dict_cmap[\"cmap\" + str(i)] = (\n",
    "            dict_cmap[\"cmap\" + str(i)] + adata.obs[\"cmap\" + str(i)].tolist()\n",
    "        )\n",
    "        \n",
    "    # Used to retrieve cell ids and fov_ids for metadata for all fovs\n",
    "    all_cells_df1 = adata.to_df()\n",
    "    all_cells_df = pd.concat([all_cells_df, all_cells_df1])\n",
    "    fovs = fovs + [fov for i in range(len(all_cells_df1))]\n",
    "\n",
    "gene_names = list(all_cells_df.columns)\n",
    "adata = adata_pred\n",
    "\n",
    "# For each cluster create csv file\n",
    "## Fov\n",
    "## CellID\n",
    "## CellType\n",
    "## Cell importance score\n",
    "## Gene importance score\n",
    "\n",
    "for k in range(0, opt.ncluster):\n",
    "    cell_id = all_cells_df.reset_index()[\"cell_ID\"]\n",
    "    gmap = adata.uns[\"gmap\" + str(k)]\n",
    "    gmap_df = pd.DataFrame(gmap, columns=gene_names)\n",
    "    genes_df = pd.concat([cell_id, gmap_df], axis=1)\n",
    "\n",
    "    # Sort with highest variance\n",
    "    top_genes = genes_df.var().sort_values(ascending=False)\n",
    "    top_genes_df = top_genes.to_frame().reset_index()\n",
    "    top_genes_df = top_genes_df.rename(columns={\"index\": \"genes\"})\n",
    "    genes = top_genes_df[\"genes\"].tolist()\n",
    "\n",
    "    cluster_metadata_df = pd.DataFrame()\n",
    "    cluster_metadata_df[\"fov\"] = fovs\n",
    "    cluster_metadata_df[\"cellID\"] = cell_id\n",
    "    cluster_metadata_df[\"cell_type\"] = gt\n",
    "    cluster_metadata_df[\"cell_imp_score\"] = dict_cmap[\"cmap\" + str(k)]\n",
    "\n",
    "    cluster_metadata_df[\"cluster\"] = outlabel\n",
    "    cluster_metadata_df[\"cx\"] = adata.obs[\"cx\"].tolist()\n",
    "    cluster_metadata_df[\"cy\"] = adata.obs[\"cy\"].tolist()\n",
    "    cluster_metadata_df[\"cx_g\"] = adata.obs[\"cx_g\"].tolist()\n",
    "    cluster_metadata_df[\"cy_g\"] = adata.obs[\"cy_g\"].tolist()\n",
    "\n",
    "    for gene in genes:\n",
    "        cluster_metadata_df[gene + \" imp score\"] = genes_df[gene]\n",
    "\n",
    "    if not os.path.exists(\n",
    "        os.path.join(\"../cluster_results_gradcam_fov1_gt1/\" + opt.dataset)\n",
    "    ):\n",
    "        os.makedirs(os.path.join(\"../cluster_results_gradcam_fov1_gt1/\" + opt.dataset))\n",
    "    cluster_metadata_df.to_csv(\n",
    "        \"../cluster_results_gradcam_fov1_gt1/\"\n",
    "        + opt.dataset\n",
    "        + \"/cluster\"\n",
    "        + str(k)\n",
    "        + \"_\"\n",
    "        + dict_cluster_map_cell[k]\n",
    "        + \".csv\"\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "delta = end_time - start_time\n",
    "sec = delta\n",
    "hours = sec / (60 * 60)\n",
    "print(\"Downstream task time in hours:\", hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4227c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "delta = end_time - start_time\n",
    "sec = delta\n",
    "hours = sec / (60 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddc73eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference in hours: 0.022173118657535976\n"
     ]
    }
   ],
   "source": [
    "print('difference in hours:', hours)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
